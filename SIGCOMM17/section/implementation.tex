\section{Implementation}
\label{sec:implementation}


%\nfactor~is implemented in C++ using a combination of DPDK \cite{dpdk}, BESS module system \cite{bess} and GRPC \cite{grpc}.
The core functionalities of \nfactor~ are implemented by about $8500$ lines of code in C++, except the code for NFs. We customize an actor library, and run our runtimes and virtual switches on Docker containers \cite{docker}. We use BESS \cite{bess} as the dataplane inter-connection tool for connecting different runtimes and virtual switches, building a virtual L2 network inside each server, and connecting each virtual L2 network to the physical L2 network connecting all the servers. The three ports in each runtime are BESS ZeroCopy VPorts - high-speed virtual ports implemented with DPDK for fetching and transmitting raw packets. With DPDK, the memory holding packet buffers is mapped directly into the address space of the runtime process, %xxx\chuan{complete},
avoiding high context switching overhead if using the traditional kernel network stack and speeding up packet handling \cite{martins2014clickos}. %By connecting the virtual L2 ethernet with the ports of runtimes,~\nfactor connects runtimes running on different server together.

\vspace{1mm}
\noindent \textbf{Resource Allocation to Runtimes.} %To use BESS VPort, the container that hosts a runtime must be configured with the highest system access right (set $--previledge=true$). Therefore each container actually could uses all the CPU cores on the server.
When launching runtimes on one server, the coordinator ensures that the worker threads of the runtimes are pinned to different CPU cores (excluding core 0 and the cores that are used by BESS), since they are busy polling threads which keep the CPU utilization to 100\%. The RPC threads of all the runtimes in the same server are collective pinned to core 0, since they sleep for most of the time waiting for RPC requests.  %The threshold for identifying whether a runtime overloads is set to 100 dropped packet on the input port of the runtime over one second.

%\subsection{Customized Actor Library}
%\label{sec:actor-library}
\vspace{1mm}
\noindent \textbf{Customized Actor Library.} We implement our own actor library, including a fast actor memory allocator, a fast actor timer and a set of C++ base classes for implementing new actors. %xxx \chuan{summarize what this library includes}.
In this actor library, due to our single-worker-thread design, local actor message transmission is directly implemented as a function call, therefore eliminating the overhead of enqueuing and dequeuing messages to and from an actor's mailbox \cite{actor-wiki}. For remote actor message passing, we assign a unique ID to each actor. A sender actor only needs to specify the receiver actor's ID and the hosting runtime's ID, and then the reliable transmission module (to be discussed soon) can correctly deliver the remote actor message to the receiver actor. We also implement a flow actor scheduler, % in the first three tasks of the worker thread. The flow actor scheduler
 which redirects both input flow packets and remote actor messages to corresponding flow actors, by looking up the flow actors using the 5-tuple.
 Even though functionalities provided by our customized actor library are much simpler than those in the existing actor frameworks \cite{akka} \cite{caf}, its simple design leads to effective actor processing overhead reduction, enabling high-speed packet processing. %Our customized actor library could not achieve complicated ac as other mature actor frameworks do \cite{akka} \cite{caf}. However, due to its simple architecture, it only imposes a small overhead when doing actor processing, therefore it is able to satisfy the high-speed packet processing requirement of modern NFV system.




%We decide to reuse BESS module systems. BESS module system is specifically designed to schedule packet processing pipelines in high-speed NFV systems, which is a perfect suit to~\nfactor runtime architecture. We port the BESS module system and BESS module scheduler to the runtime and implement all the actor processing tasks as BESS modules. All the implemented BESS modules are  scheduled inside worker thread to achieve flow actor scheduling, packet processing over the service chain and remote actor message transmission.

%\chuan{moved to runtime section} The worker thread routinely carries out the following four tasks. (i) Poll dataplane packets from intput/output port, run a actor schedule to schedule flow actors to process these packets and sends the packets out from the output/input port after flow actors finishes processing the packets. (ii) Poll packets from control ports, reconstruct packet stream into remote actor messages and send the actor messages to the receiver actors. (The first task also carries out similar processing as the second task because remote messages are also sent to input/output port as shown in Sec.~\ref{sec:resilience}). (iii) Run the liason actor to process RPC requests sent from the coordinator and dispatches flow migration initiation messages to active flow actors in the runtime. (iv) Fetch remote actor messages from a queue that are enqueued by actors during the execution of previous three tasks and send the remote actor messages out from the three ports.

%\begin{itemize}

%\item The first/second pipeline polls packets from the input/output port, runs actor scheduler on these packets and sends the packets out from the output/input port.

%\item The third pipeline polls packets from control ports, reconstruct packet stream into remote actor messages and send the actor messages to the receiver actors. (The first/second pipeline also carries out this processing because remote messages are also sent to input/output port \ref{}).

%\item The fourth pipeline schedules coordinator actor to execute RPC requests sent from the controller. In particular, coordinator actor updates the configuration information of other runtimes in the cluster and dispatches flow migration initiation messages to active flow actors in the runtime.

%\item When processing the previous four pipelines, the actors may send remote actor messages. These messages are placed into ring buffers \ref{}. The fifth pipeline fetches remote actor messages from these ring buffers and sends remote actor messages out from corresponding ports.

%\end{itemize}

%The runtime uses BESS scheduler to schedule these 5 pipelines in a round-rubin manner to simulate a time-sharing scheduling.



%\subsection{Reliable Message Passing Module}
%\label{sec:ReliableMsgPassing}
\vspace{1mm}
\noindent \textbf{Reliable Remote Actor Message Passing.}
Actor messages passed among flow actors on different runtimes should be reliably delivered. We build a customized reliable message passing module, which inserts those messages into a reliable packet stream for transmission. % to the other end, the reliable message passing encodes messages into reliable packet streams.
The module creates one ring buffer for each remote runtime and virtual switch. When a flow actor on this runtime sends a remote actor message, the module creates a packet, copies the content of the message into the packet and then enqueues the packet into the respective ring buffer. %A message may be split into several packets. %and different messages do not share packets.
These packets are configured with a sequential number each, and appended with a special header to differentiate them from dataplane packets.
The worker thread dequeues these packets from the ring buffers, and sends them to respective remote runtimes. A remote runtime acknowledges receipt of such packets. Retransmission is fired in case that the acknowledgement for a packet is not received after a configurable timeout (\eg, 10 times the RTT).

Since our goal is to reliably transmit remote actor messages over an inter-connected L2 network, we do not use user-level TCP \cite{jeong2014mtcp}, %\chuan{reference missing in .bib, your project code url is also missing in .bib},
which may impose much more processing overhead for reconstructing byte streams into messages.
In addition, packet-based reliable message passing provides additional benefits during flow migration and replication. Because the response in 2nd request-response step of flow migration is sent as a packet using the same path as the dataplane packets, reliable actor message passing enables us to implement loss-avoidance migration with ease (Sec.~\ref{sec:migration}).
%I will remove this claim, I forgot that we still need to do packet copy.
%During flow replication, we can directly send the output packet as a message to the replica actor, without the need of additional packet copy \chuan{I do not get what additional packet copy might be needed}.

%\subsection {Worker Thread}
%\label{sec:BessModuleSys}
\vspace{1mm}
\noindent \textbf{Worker and RPC Threads.}
The worker thread on a runtime polls packets from the ports, schedules flow actors and transmits remote actor messages (Sec.~\ref{sec:runtime}). To schedule these tasks efficiently, we implement them as BESS modules and use the BESS module scheduler, which schedules these modules in a round-robin fashion. Each worker thread is pinned to one CPU core to avoid overhead due to OS scheduling. % and improve packet processing performance.
 The RPC thread on each runtime is implemented with GRPC \cite{grpc}, and the RPC requests are sent over a reliable TCP connection between a runtime and the coordinator.

%I'm about to move it to evaluation secition.
%\subsection{New Applications}

%We also build several applications based on \nfactor, which exploits its lightweight, distributed flow migration capability to achieve useful functionalities. %, including live NF update, reduce output bandwidth for deduplication NF and ensure reliable and safe MPTCP processing. We evaluate and demonstrate these new applications in our evaluation.

%\noindent\textbf{Live NF update.} %NF often needs to be updated (\eg, software version, important NF configuration files), existing systems would typically migrate flows away, stop the NF and then relaunching it.
% \nfactor~can achieve dynamic NF update (\eg, software version, important NF configuration files) without interfering active flows running on a runtime by dynamically migrating the flows out to a replica runtime, performing update and migrating the flows back. We show in the evaluation that the dynamic update ensures no active flows are dropped during the update.


%\noindent\textbf{MPTCP sub-flow processing.} When a MPTCP \cite{} flow traverses an NFV system, its sub-flows may be sent to different NF instances for processing. Some network functions require all subflows to be processed by the same instance, \eg, xxx \chuan{is this the problem of subflows being processed by different instances?}. %because they have different flow-5-tuple and appear as different flows on the virtual switch.
%In \nfactor, we can add a MPTCP sub-flow detection function to each flow actor, such that when the flow actor processes the first packet of a flow, it can check whether it belongs to a MPTCP flow. If so, the flow actor performs a consistent hashing using the MPTCP header to decide a migration target runtime in the cluster, and migrates the flow to the target. In this way, different sub-flows belonging to the same MPTCP flow can be processed by the same flow actor. This is hard to be achieved in existing NFV systems without signicant central coordination. %, since flows can not initiate active migration request by themselves.

%The final application is similar with MPTCP application, which reduces the output bandwidth for deduplication.
%\noindent\textbf{Efficient Flow Deduplication.} Flow deduplication, as an important functionality of WAN optimizers \chuan{add citation}, is useful for reducing bandwidth consumption due to transfer redundant data across different flows. \nfactor~can efficiently support flow deduplication: when a runtime receives a flow, it checks for duplicated content contained in the flow packet \chuan{how does it check duplicated content among different flows received on different runtimes?}. In case it finds out the duplication, the flow actor initiates a migration to move the flow to a specific runtime \chuan{how to decide this specific runtime}. In this way, duplicated flows can be processed on the same runtime, eliminating the output bandwidth on each runtime \chuan{why processing duplicated flows on the same runtime can reduce output bandwidth? Are they processed by the same flow actor? what if the flows are partially duplicated but not entirely?}. This is hard to be achieved in existing NFV systems because xxx \chuan{give the reason}.
